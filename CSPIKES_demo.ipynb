{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demostration of the C-SPIKES algorithm for calcium imaging spike inference\n",
    "## The algorithm demonstration here reflects the exposition as in \"Precise calcium-to-spike inference using biophysical generative models\" from Broussard et al.\n",
    "### The first two cells in this notebook must be run prior to running the PGAS algorithm cells (labeled below)\n",
    "### Other blocks can be run in any order desired as long as the flags in the following code cell are properly set. See below for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import libraries, including the python-bound c++ library where the PGAS algorithm is implemented. We will also introduce a few helper methods that handle data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing project packages and required libraries\n",
    "import numpy as np\n",
    "import build.pgas_bound as pgas\n",
    "from src.c_spikes.syn_gen import synth_gen\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import os\n",
    "\n",
    "#Setting flags for what to calculate on this run\n",
    "recalc_pgas = True # this runs the particle Gibbs sampler to extract spike times and cell parameters\n",
    "recalc_Cparams = True # this runs the particle Gibbs sampler with known spike times to extract cell parameters\n",
    "recalc_synth = True # this runs the synthetic data generation code to create new synthetic data\n",
    "retrain_and_infer = True # this runs the cascade training and inference code to train a new model and infer spikes on the original data\n",
    "\n",
    "# Utility-type methods\n",
    "## Code to convert spike times to a binary vector\n",
    "def spike_times_2_binary(spike_times,time_stamps):\n",
    "    # initialize the binary vector\n",
    "    binary_vector = np.zeros(len(time_stamps), dtype=int)\n",
    "\n",
    "    # get event times within the time_stamps ends\n",
    "    good_spike_times = spike_times[(spike_times >= time_stamps[0]) & (spike_times <= time_stamps[-1])]\n",
    "    \n",
    "    # Find the nearest element in 'a' that is less than the elements in 'b'\n",
    "    for event_time in good_spike_times:\n",
    "        # Find indices where 'a' is less than 'event_time'\n",
    "        valid_indices = np.where(time_stamps < event_time)[0]\n",
    "        if valid_indices.size > 0:\n",
    "            nearest_index = valid_indices[-1]  # Taking the last valid index\n",
    "            binary_vector[nearest_index] += 1\n",
    "\n",
    "    return binary_vector\n",
    "\n",
    "# For opening the janelia datasets\n",
    "def open_Janelia_1(j_path):\n",
    "    all_data = sio.loadmat(j_path)\n",
    "    dff = all_data['dff']\n",
    "    time_stamps = all_data['time_stamps']\n",
    "    spike_times = all_data['ap_times'] \n",
    "\n",
    "    return time_stamps, dff, spike_times\n",
    "\n",
    "# For working with the PGAS output state trajectories\n",
    "def unroll_dat_files(dat_file):\n",
    "    '''\n",
    "    PGAS contain the following output variables:\n",
    "    -B = basline drift, brownian\n",
    "    -S = discretized spike number per time bin\n",
    "    -C = \"calcium\" value - really more akin to a DFF-like metric\n",
    "    -Y = original data (not included in PGBAR output)\n",
    "    '''\n",
    "    \n",
    "\n",
    "    data = np.genfromtxt(dat_file, delimiter=',', skip_header=1)\n",
    "    #Dealing out data\n",
    "    index = data[:,0]\n",
    "    B = data[:,2]\n",
    "    S = data[:,3]\n",
    "    C = data[:,4]\n",
    "    \n",
    "    #Note that files produced by PGBAR (rather than the more general PGAS) lack Y - for now not including it\n",
    "    try:\n",
    "        Y = data[:,5]\n",
    "    except:\n",
    "        Y = np.nan\n",
    "    \n",
    "    return index,B,S,C,Y\n",
    "\n",
    "# For calculating noise levels in the data\n",
    "# This method is based on one from Rupprecht et al. 2021 CASCADE paper\n",
    "def calculate_standardized_noise(dff,frame_rate):\n",
    "    noise_levels = np.nanmedian(np.abs(np.diff(dff, axis=-1)), axis=-1) / np.sqrt(frame_rate)\n",
    "    return noise_levels * 100     # scale noise levels to percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll take some sample jGCaMP8f data reported in Zhang et al 2023 with data retrieved from Márton Rózsa's dandi dataset 000168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll load in the original data as a numpy array\n",
    "#janelia_file = \"jGCaMP8f_ANM471993_cell03\" (high SNR excitatory)#\"jGCaMP8f_ANM478349_cell06\" (low SNR inhibitory)\n",
    "janelia_file = \"jGCaMP8f_ANM471993_cell03\"\n",
    "filename = os.path.join(\"gt_data\",janelia_file+\".mat\")\n",
    "\n",
    "time,data,spike_times = open_Janelia_1(filename)\n",
    "time1 = np.float64(time[0,1000:2000])\n",
    "time1 = time1.copy()\n",
    "data1 = np.float64(data[0,1000:2000])\n",
    "data1 = data1.copy()\n",
    "binary_spikes = np.float64(spike_times_2_binary(spike_times,time1))\n",
    "\n",
    "## Setting up parameters for the particle gibbs sampler\n",
    "tag=\"test\"\n",
    "Gparam_file=\"src/spike_find/pgas/20230525_gold.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PGAS for spike time estimates\n",
    "## Now we are ready to run the PGAS algorithm with the biophysical GCaMP model as its generative kernel\n",
    "## This cell runs the Particle Gibbs sampler for 300 iterations\n",
    "## The output file is a viewable plot of the average spike state trajectory (trace) compared to ground truth spike times (depicted as vertical dashed lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters for the particle gibbs sampler\n",
    "analyzer = pgas.Analyzer(\n",
    "    time=time1,\n",
    "    data=data1,\n",
    "    constants_file=\"parameter_files/constants_GCaMP8_soma.json\",\n",
    "    output_folder=\"results\",\n",
    "    column=1,\n",
    "    tag=tag,\n",
    "    niter=300,\n",
    "    append=False,\n",
    "    verbose=1,\n",
    "    gtSpikes=binary_spikes,\n",
    "    has_gtspikes=False,\n",
    "    maxlen=1000, \n",
    "    Gparam_file=Gparam_file,\n",
    "    seed=2\n",
    ")\n",
    "\n",
    "## Run the sampler\n",
    "analyzer.run()\n",
    "\n",
    "## Load files containing the results to plot\n",
    "pgas_out_file = os.path.join('results','traj_samples_'+tag+'.dat')\n",
    "index, B, S, C, Y = unroll_dat_files(pgas_out_file)\n",
    "\n",
    "## Get the mean trajectory with specified burnin\n",
    "trajs = [];spks = [];avg_spks = []\n",
    "spks.append(S.reshape((-1,np.sum(index==0))).T)\n",
    "avg_spks.append(np.mean(spks[i][:,-100:-1],axis=1))\n",
    "\n",
    "## Plotting the results\n",
    "fig, axes = plt.subplots(1,1, figsize=(3, 3))\n",
    "axes.plot(time1, data1, label='Data')\n",
    "axes.plot(time1, avg_spks, label='PGAS')\n",
    "axes.set_xlabel(\"Time (s)\")\n",
    "# adding gt spikes\n",
    "for spike in spike_times:\n",
    "    axes.axvline(spike,ls='--',alpha=0.5,linewidth=0.8)\n",
    "\n",
    "## Save figure to results\n",
    "plt.show()\n",
    "fig.savefig(os.path.join('results',tag+'_trajs.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PGAS for Cparam extraction\n",
    "## Our PGAS implementation can be run either with no knowledge of spike times, or (as in this cell) with spike times given to improve estimates of the underlying cell parameters\n",
    "## If you wish to run later cells without performing cell parameter estimation, that can be done by setting the flag \"recalc_Cparams\" to false in the first code cell of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recalc_Cparams:\n",
    "    analyzer = pgas.Analyzer(\n",
    "        time=time1,\n",
    "        data=data1,\n",
    "        constants_file=\"parameter_files/constants_GCaMP3_soma.json\",\n",
    "        output_folder=\"pgas_output\",\n",
    "        column=1,\n",
    "        tag=tag,\n",
    "        niter=2,\n",
    "        append=False,\n",
    "        verbose=1,\n",
    "        gtSpikes=binary_spikes,\n",
    "        has_gtspikes=True,\n",
    "        maxlen=1000, \n",
    "        Gparam_file=Gparam_file,\n",
    "        seed=2\n",
    "    )\n",
    "\n",
    "    ## Run the sampler\n",
    "    analyzer.run()\n",
    "\n",
    "    ## Return cell parameter estimate distributions\n",
    "    parameter_samples = analyzer.get_parameter_estimates()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Synthetic data generation\n",
    "## Cparams extracted using PGAS can be used to run the Biphysical cell model forward.\n",
    "## That process is demonstrated here. \n",
    "### To run this seciton, \"recalc_synth\" should be set to True\n",
    "### Note that if you set \"recalc_Cparams\" as True, you must run cell 2 prior to this one to allow PGAS to extract cell parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Cparameter file if not recalculated\n",
    "if not recalc_Cparams:\n",
    "    ## Param file location\n",
    "    param_sample_file = os.path.join(\"sample_pgas_output\",\"param_samples_\"+tag+\".dat\")\n",
    "    ## Opening the saved parameter samples for use as Cparams\n",
    "    parameter_samples = np.loadtxt(param_sample_file,delimiter=',',skiprows=1)\n",
    "\n",
    "# Prepare Cparams calculation from parameter estimates - less than 100 samples for testing\n",
    "burnin = 100 if np.size(parameter_samples,0) > 100 else 0\n",
    "parameter_samples = parameter_samples[burnin:,0:6]\n",
    "print(\"mean of samples\")\n",
    "print(np.mean(parameter_samples,axis=0))\n",
    "\n",
    "# Construct synthetic dataset\n",
    "if recalc_synth:\n",
    "    # Create synthetic data\n",
    "    ## Load parameters into the GCaMP model to use for synthetic data creation\n",
    "    Cparams = np.mean(parameter_samples,axis=0)\n",
    "    Gparams = np.loadtxt(Gparam_file)\n",
    "    gcamp = pgas.GCaMP(Gparams,Cparams)\n",
    "\n",
    "    ## Generate synthetic data from the PGAS-derived cell paramters \n",
    "    # Now making broader spike pulls\n",
    "    nominal_rates = np.array([1,1.1,1.5,2,2.5,3,3.5,4,4.5,5])#\n",
    "    for rate in nominal_rates:\n",
    "        synth = synth_gen.synth_gen(plot_on=False,GCaMP_model=gcamp,\\\n",
    "            spike_rate=rate,cell_params=Cparams,tag=tag,use_noise=True)\n",
    "        synth.generate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using synthetic data to train a CASCADE model for spike inference\n",
    "## Either the output from 3 or the sample synthetic data (included here) can be used as the training dataset\n",
    "## Inference is run on sample data from Rózsa's jGCaMP8f dataset.\n",
    "### To run this cell's contents, the \"retrain_and_infer\" flag should be set to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrain_and_infer:\n",
    "    ## Package checks for cascade\n",
    "    print(\"Current directory: {}\".format(os.getcwd()))\n",
    "    from src.spike_find.cascade2p import checks, utils, cascade\n",
    "    print(\"\\nChecks for packages:\")\n",
    "    checks.check_packages()\n",
    "\n",
    "    ## Train a cascade model using the synthetic dataset\n",
    "    # First get the data and noise level we're training against\n",
    "    #data = np.loadtxt(data_file).transpose()\n",
    "    time_stamps = time[0,:]\n",
    "    fluo_data = data[0,:]\n",
    "    frame_rate = 1/np.mean(np.diff(data[0,:]))\n",
    "    noise_level = utils.calculate_noise_levels(fluo_data,frame_rate)\n",
    "\n",
    "    # Set configurations file for sample training\n",
    "    synthetic_test = f\"synth_{tag}\"\n",
    "    synthetic_training_dataset = os.path.join(f\"synth_{tag}\")\n",
    "    cfg = dict( \n",
    "        model_name = tag,    # Model name (and name of the save folder)\n",
    "        sampling_rate = 30,    # Sampling rate in Hz (round to next integer)\n",
    "        \n",
    "        training_datasets = [\n",
    "                synthetic_training_dataset\n",
    "                            ],\n",
    "        \n",
    "        noise_levels = [noise for noise in range(2,8)],#\n",
    "        \n",
    "        smoothing = 0.05,     # std of Gaussian smoothing in time (sec)\n",
    "        causal_kernel = 0,   # causal ground truth smoothing kernel\n",
    "        verbose = 1,\n",
    "            )\n",
    "    \n",
    "    # ## save parameter as config.yaml file - TODO: make cascade overwrite configs on this call\n",
    "    print(cfg['noise_levels'])\n",
    "\n",
    "    cascade.create_model_folder( cfg )\n",
    "\n",
    "    ## Train a model based on config contents\n",
    "    #from spike_find.cascade2p import models\n",
    "    model_name = cfg['model_name']\n",
    "    cascade.train_model( model_name )\n",
    "\n",
    "    # ## Use trained model to perform inference on the original dataset\n",
    "    from spike_find.cascade2p.utils_discrete_spikes import infer_discrete_spikes\n",
    "    spike_prob = cascade.predict(model_name, np.reshape(fluo_data, (1, len(fluo_data))))\n",
    "\n",
    "    # Separate Python file that organizes model names \n",
    "    ## Saving routine\n",
    "    save_dir = \"results\"\n",
    "    os.makedirs(save_dir,exist_ok=True)\n",
    "    save_path = os.path.join(save_dir,f\"{tag}_syn_trained_CASCADE_output.mat\")\n",
    "    sio.savemat(save_path,{'spike_prob':spike_prob,'time_stamps':time_stamps,'dff':fluo_data,'cfg':cfg})#\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
